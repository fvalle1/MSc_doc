
<h1 id="app:hsbm">Hierarchical stochastic block model</h1>
<p>The algorithm called hierarchic Stochastic Block Model (hSBM) from <span class="citation" data-cites="gerlach2018network"></span> is hereby summed up.</p>
<p>The first step of hierarchical stochastic block model, as discussed in <span class="citation" data-cites="peixoto2014efficient"></span>, is to create a bipartite network <span class="math inline"><em>G</em></span> with two kinds of nodes: <strong>words</strong> and <strong>documents</strong>. Every time a word <span class="math inline"><em>w</em></span> is present in a document <span class="math inline"><em>d</em></span> an edge <span class="math inline"><em>e</em><sub><em>w</em><em>d</em></sub></span> is created. If a word count in the entire corpus is under a certain threshold, that word could be ignored (this is not done in this work). The aim is to find a partition <span class="math inline"><em>b</em> ∈ {<em>b</em><sub><em>i</em></sub>}</span> with <span class="math inline"><em>B</em> = |{<em>b</em><sub><em>i</em></sub>}|</span> blocks.</p>
<p>This model belongs to the so called <em>generative models</em>: given the data, the model should generate a network <span class="math inline"><em>G</em></span> with probability <span class="math inline"><em>P</em>(<em>G</em>|<em>θ</em>, <em>b</em>)</span>, where <span class="math inline"><em>b</em></span> is the partition and <span class="math inline"><em>θ</em></span> any additional parameter of the model.</p>
<p>Using the well-known Bayes theorem one could estimate the probability that an observed network was generated by the partition <span class="math inline"><em>b</em></span>: <br /><span class="math display">$$\label{eq:PbonG}
  P(b,\theta|G)=\frac{P(G|b,\theta)\overbrace{P(b,\theta)}^{prior}}{\underbrace{P(G)}_{\sum_{\theta}P(G|\theta, b)P(\theta, b)}}$$</span><br /> defining the amount of information needed to describe the data as the description length <br /><span class="math display"><em>Σ</em> =  − <em>l</em><em>n</em><em>P</em>(<em>G</em>|<em>b</em>, <em>θ</em>) − <em>l</em><em>n</em><em>P</em>(<em>b</em>, <em>θ</em>)</span><br /> the <a href="#eq:PbonG" data-reference-type="ref" data-reference="eq:PbonG">[eq:PbonG]</a> can be written as <span class="math inline">$\frac{e^{-\Sigma}}{P(G)}$</span>, so maximizing the posterior probability is equivalent to minimize the description length <a href="#eq:descriptionlenght" data-reference-type="ref" data-reference="eq:descriptionlenght">[eq:descriptionlenght]</a>. The probability of obtaining a graph from a set of parameters is <span class="math inline">$P(G|b,\theta)=\frac{1}{\Omega(A,\{n_r\})}$</span>, where <span class="math inline"><em>Ω</em>(<em>A</em>, {<em>n</em><sub><em>r</em></sub>})</span> is the number of graphs that is possible to generate with adjacency matrix <span class="math inline"><em>A</em></span> and <span class="math inline">{<em>n</em><sub><em>r</em></sub>}</span> the counts distribution of block partition <span class="math inline">{<em>b</em><sub><em>i</em></sub>}</span> sizes. Note that the description length depends on the entropy: <br /><span class="math display"><em>Σ</em> = <em>S</em> − <em>l</em><em>n</em><em>P</em>(<em>θ</em>),</span><br /> So minimize <span class="math inline"><em>S</em></span> one minimizes <span class="math inline"><em>Σ</em></span> and maximize the posterior <span class="math inline"><em>P</em>(<em>θ</em>|<em>G</em>)</span>. In case of a weighted network the likelihood becomes <span class="math inline"><em>P</em>(<em>G</em>, <em>x</em>|<em>b</em>, <em>θ</em>)</span>, where <span class="math inline"><em>x</em></span> are the weights.</p>
<h5 id="algorithm">Algorithm</h5>
<p><br />
First of all a <span class="math inline"><em>B</em> × <em>B</em></span> matrix is created. The entry <span class="math inline"><em>e</em><sub><em>r</em><em>s</em></sub></span> of this matrix represents the number of links between nodes of group <span class="math inline"><em>r</em></span> and nodes of group <span class="math inline"><em>s</em></span>, with <span class="math inline"><em>r</em>, <em>s</em> ∈ {<em>b</em><sub><em>i</em></sub>}</span>. At the beginning <span class="math inline"><em>B</em></span> groups are formed at random and the initial <span class="math inline"><em>B</em></span> is a hyper-parameter of the model.</p>

<figure>
<embed src="pictures/topic/peixioto_ers.pdf" id="fig:hsbm-ers" /><figcaption>Example of an edge’s matrix from <span class="citation" data-cites="peixoto_graph-tool_2014"></span>.<span label="fig:hsbm-ers"></span></figcaption>
</figure>
<p>It is useful to define a traditional entropy: <br /><span class="math display">$$\label{eq:hSBMentropyt}
  S_t=\frac{1}{2}\Sigma_{r,s} n_rn_sH\left(\frac{e_{rs}}{n_rn_s}\right)$$</span><br /> where <span class="math inline"><em>n</em><sub><em>r</em></sub></span> is the number of nodes in groups <span class="math inline"><em>r</em></span>, <span class="math inline"><em>e</em><sub><em>r</em><em>s</em></sub></span> is the number of edges between nodes of group <span class="math inline"><em>r</em></span> and nodes of group <span class="math inline"><em>s</em></span>, and <span class="math inline"><em>H</em>(<em>x</em>) =  − <em>x</em><em>l</em><em>n</em>(<em>x</em>) − (1 − <em>x</em>)<em>l</em><em>n</em>(1 − <em>x</em>)</span>. This entropy is equivalent to the micro-canonical entropy of a system with <span class="math inline"><em>Ω</em>(<em>A</em>, {<em>n</em><sub><em>r</em></sub>})</span> accessible states <span class="math inline"><em>S</em><sub><em>t</em></sub> = <em>L</em><em>n</em><em>Ω</em></span>.</p>
<p>The algorithm uses a Markov Chain Monte Carlo to minimize this entropy. Made it simple, at each step a node changes block and the new configuration is accepted if <span class="math inline"><em>S</em></span> is decreased.</p>
<p>Note that <a href="#eq:hSBMentropyt" data-reference-type="ref" data-reference="eq:hSBMentropyt">[eq:hSBMentropyt]</a> can be corrected taking care of degree distribution obtaining corrected entropy <span class="math inline"><em>S</em><sub><em>c</em></sub></span> <br /><span class="math display">$$S_c=-\Sigma_{r,s}\frac{e_{rs}}{2}-\Sigma_k
  N_kln(k!)-\frac{1}{2}\Sigma_{r,s}e_{rs}ln\left(\frac{e_{rs}}{e_re_s}\right),$$</span><br /> being <span class="math inline"><em>k</em></span> the degree distribution.</p>
<h5 id="how-to-change-group-of-a-node">How to change group of a node?</h5>
<p><br />
At each step according to <span class="citation" data-cites="peixoto2014efficient"></span> node <span class="math inline"><em>i</em></span> can change group from <span class="math inline"><em>r</em></span> to <span class="math inline"><em>s</em></span> with a probability <br /><span class="math display">$$\label{eq:Prst}
  P(r\to s|t)=\frac{e_{ts}+\epsilon}{e_t+\epsilon B}$$</span><br /> where <span class="math inline"><em>j</em></span> is a random neighbour of <span class="math inline"><em>i</em></span>: <span class="math inline"><em>j</em> ∈ <em>N</em><sub><em>i</em></sub></span>, <span class="math inline"><em>t</em> ∈ {<em>b</em><sub><em>j</em></sub>}</span> its block as defined in <span class="citation" data-cites="peixoto2014efficient"></span>. <span class="math inline"><em>ϵ</em></span> is a parameter that according to <span class="citation" data-cites="peixoto2017nonparametric"></span> it hasn’t significant impact in the algorithm, provided it is sufficiently small. Equation <a href="#eq:Prst" data-reference-type="ref" data-reference="eq:Prst">[eq:Prst]</a> can be rewritten as <br /><span class="math display">$$P(r\to s|t)=(1-R_t)\frac{e_{ts}}{e_t}+\frac{R_t}{B}$$</span><br /> defining <span class="math inline">$R_t=\frac{\epsilon B}{e_t + \epsilon B}$</span>. The simulation consists of four steps: for each node <span class="math inline"><em>i</em></span></p>
<ul class="incremental">
<li><p>a node <span class="math inline"><em>j</em></span> is chosen from <span class="math inline"><em>i</em></span>’s neighbours, the group of <span class="math inline"><em>j</em></span> is called <span class="math inline"><em>t</em></span>,</p></li>
<li><p>a random group <span class="math inline"><em>s</em></span> is selected,</p></li>
<li><p>move of node <span class="math inline"><em>i</em></span> to group <span class="math inline"><em>s</em></span> is accepted with probability <span class="math inline"><em>R</em><sub><em>t</em></sub></span>,</p></li>
<li><p>if <span class="math inline"><em>s</em></span> is not accepted, a random edge <span class="math inline"><em>e</em></span> is chosen from group <span class="math inline"><em>t</em></span> and node <span class="math inline"><em>i</em></span> is assigned to the endpoint of <span class="math inline"><em>e</em></span> which is not in <span class="math inline"><em>t</em></span>.</p></li>
</ul>
<p>This steps mime probability <a href="#eq:Prst" data-reference-type="ref" data-reference="eq:Prst">[eq:Prst]</a>; note that for <span class="math inline"><em>ϵ</em> → ∞</span> this gives a uniform probability.</p>
<p>To enchant the probability to find a minimum, a bounce of these moves is made, only the set of moves with the minimum <span class="math inline"><em>S</em></span> is accepted. Moreover, in order to remove eventual biases due to the initial configuration the model is run with different initial states, then the final state with the minimal entropy is selected.</p>
<h5 id="how-many-blocks-b">How many blocks <span class="math inline"><em>B</em></span>?</h5>
<p><br />
Note that the number of blocks <span class="math inline"><em>B</em></span> is a free parameter and must be inferred as described in <span class="citation" data-cites="peixoto2017nonparametric"></span>. This implies a slight modification of the algorithm such that it became possible to admit the creation of a new group. When a group <span class="math inline"><em>s</em></span> is chosen, the algorithm can now accept a <strong>new group</strong> and <a href="#eq:Prst" data-reference-type="ref" data-reference="eq:Prst">[eq:Prst]</a> became <br /><span class="math display">$$\label{eq:PrstB1}
  P(r\to s)=\Sigma_t P(t|i)\frac{e_{ts}+\epsilon}{e_t+\epsilon (B+1)}$$</span><br /> being <span class="math inline">$P(t|i)=\Sigma_j\frac{A_{ij}\delta(b_j, t)}{k_i}$</span> the fraction of neighbours of <span class="math inline"><em>i</em></span> belonging to group <span class="math inline"><em>t</em></span>, <span class="math inline"><em>e</em><sub><em>t</em></sub></span> the number of edges in group <span class="math inline"><em>t</em></span>, <span class="math inline"><em>k</em><sub><em>i</em></sub></span> the degree, and <span class="math inline"><em>b</em><sub><em>j</em></sub></span> groups.</p>
<p>Using this modification it is now possible to add new groups and <span class="math inline"><em>B</em></span> is no longer a parameter.</p>
<h5 id="how-to-find-hierarchical-layers">How to find hierarchical layers?</h5>
<p><br />
After the algorithm is run, one would to add a new hierarchic level, this is done considering the <span class="math inline"><em>B</em></span> groups as nodes and repeating the process. As done before a matrix of edges like <a href="#fig:hsbm-ers" data-reference-type="ref" data-reference="fig:hsbm-ers">[fig:hsbm-ers]</a> is created, where edges are considered between groups of the previous layer. The posterior probability became <br /><span class="math display">$$\label{eq:posteriorL}
  P(\{b_l\}|A)=\frac{P(A|\{b_l\})P(\{b_l\})}{P(A)}=\prod_l^L P(b_l|e_l,b_{l-1})$$</span><br /> where <span class="math inline"><em>l</em> = 0…<em>L</em></span> is the layer, <span class="math inline"><em>A</em></span> the audience matrix, <span class="math inline"><em>b</em><sub><em>i</em></sub></span> blocks. Note that <span class="math inline"><em>e</em><sub>0</sub> = <em>A</em></span> and <span class="math inline"><em>B</em><sub><em>L</em></sub> = 1</span>. Maximising <a href="#eq:posteriorL" data-reference-type="ref" data-reference="eq:posteriorL">[eq:posteriorL]</a> gives the correct number of layers.</p>
<p>Adding a layer is done in 3 steps described in <span class="citation" data-cites="peixoto2014hierarchic"></span>:</p>
<ul class="incremental">
<li><p>find <span class="math inline"><em>B</em><sub><em>l</em></sub> ∈ [<em>B</em><sub><em>l</em> − 1</sub>, <em>B</em><sub><em>l</em> + 1</sub>]</span> by bisection,</p></li>
<li><p>a layer <span class="math inline"><em>l</em></span>,</p></li>
<li><p><span class="math inline"><em>l</em></span> and linking nodes from layer <span class="math inline"><em>l</em> − 1</span> directly to groups of layer <span class="math inline"><em>l</em> + 1</span>.</p></li>
</ul>
<p>One marks initially all levels as <em>not done</em> and starts at the top-level <span class="math inline"><em>l</em> = <em>L</em></span> <span class="citation" data-cites="peixoto2014hierarchic"></span>. For the current level <span class="math inline"><em>l</em></span>, if it is marked <em>done</em> it is skipped and one moves to the level <span class="math inline"><em>l</em> − 1</span>. Otherwise, all three moves are attempted. If any of the moves succeeds in decreasing the description length <span class="math inline"><em>Σ</em></span> (<a href="#eq:descriptionlenght" data-reference-type="ref" data-reference="eq:descriptionlenght">[eq:descriptionlenght]</a>), one marks the levels <span class="math inline"><em>l</em> − 1</span> and <span class="math inline"><em>l</em> + 1</span> (if they exist) as <em>not done</em>, the level <span class="math inline"><em>l</em></span> as <em>done</em>, and one proceeds (if possible) to the upper level <span class="math inline"><em>l</em> + 1</span>, and repeats the procedure. If no improvement is possible, the level <span class="math inline"><em>l</em></span> is marked as <em>done</em> and one proceeds to the lower level <span class="math inline"><em>l</em> − 1</span>. When the lowest level <span class="math inline"><em>l</em> = 0</span> is reached and cannot be improved, the algorithm ends.</p>
<h5 id="overlapping-partitions">Overlapping partitions</h5>
<p><br />
As described in <span class="citation" data-cites="peixoto2015model"></span> one of the advantages of this approach is that it is possible to let a node belonging to multiple groups. In this case <span class="math inline"><em>b</em><sub><em>i</em></sub></span> becomes <span class="math inline">$\vec{b_i}$</span>, with component <span class="math inline"><em>b</em><sub><em>i</em><em>r</em></sub> = 1</span> if node <span class="math inline"><em>i</em></span> is in group <span class="math inline"><em>r</em></span>, <span class="math inline">0</span> otherwise. The number of <span class="math inline">1</span>s in vector <span class="math inline">$\vec{b_i}$</span> is called <span class="math inline">$d_i=|\vec{b_i}|$</span>.</p>
<p>The probability of having a graph <span class="math inline"><em>G</em></span> being generated from an adjacency matrix <span class="math inline"><em>A</em></span> and a partition <span class="math inline">$\{\vec{b_i}\}$</span> is <br /><span class="math display">$$P(G|A,\{\vec{b_i}\})=\frac{1}{\Omega}$$</span><br /> if <span class="math inline"><em>Ω</em></span> is the number of possible graphs. Entropy <a href="#eq:hSBMentropyt" data-reference-type="ref" data-reference="eq:hSBMentropyt">[eq:hSBMentropyt]</a> is <span class="math inline"><em>S</em><sub><em>t</em></sub> = <em>L</em><em>n</em><em>Ω</em></span>. This corresponds to an augmented graph generated via a non overlapping block model with <span class="math inline"><em>N</em>′ = <em>Σ</em><sub><em>r</em></sub><em>n</em><sub><em>r</em></sub> &gt; <em>N</em></span> nodes and the same adjacency matrix <span class="math inline"><em>A</em></span>.</p>
<p>First of all, it is necessary to sample the distribution of mixture sizes <span class="math inline"><em>P</em>({<em>n</em><sub><em>d</em></sub>})</span> where <span class="math inline"><em>n</em><sub><em>d</em></sub></span> is the number of nodes which mixture has got size <span class="math inline"><em>d</em></span>, <span class="math inline"><em>n</em><sub><em>d</em></sub> ∈ [0, <em>N</em>]</span> and <span class="math inline"><em>d</em> ∈ [0, <em>D</em>]</span> (typically <span class="math inline"><em>D</em> = <em>B</em></span> and in the non-overlapping case <span class="math inline"><em>D</em> = 1</span>), this is done by sampling uniformly from <br /><span class="math display">$$P(\{n_d\}|B)=\left(\binom{D}{N}\right)^{-1}$$</span><br /> which is probability of having <span class="math inline"><em>n</em></span> nodes whose mixture has size <span class="math inline"><em>d</em></span>. <span class="math inline">$\left(\binom{B}{N}\right)$</span> is the number of histograms with area <span class="math inline"><em>N</em></span> and <span class="math inline"><em>B</em></span> distinguishable bins. <span class="math inline"><em>B</em> − 1</span> can be used instead of <span class="math inline"><em>B</em></span> to avoid node with no group, in this case <span class="math inline"><em>d</em> ∈ [1, <em>B</em>]</span>.</p>
<p>Given the mixture sizes, the distribution of node membership is sampled from <br /><span class="math display">$$P(\{d_i\}|\{n_d\})=\frac{\prod_{d} n_d!}{N!}.$$</span><br /></p>
<p>At this point for each set of nodes with <span class="math inline"><em>d</em><sub><em>i</em></sub> = <em>d</em></span> it is necessary to sample <span class="math inline"><em>n</em><sub><em>b⃗</em></sub></span>: the number of nodes with a particular mixture <span class="math inline"><em>b⃗</em></span>. It is sampled from <br /><span class="math display">$$P(\{n_{\vec{b}}\}_d|n_d)=\left(\binom{\binom{D}{d}}{n_d}\right)^{-1},$$</span><br /> next all mixtures <span class="math inline">$\vec{b_i}$</span> of size <span class="math inline"><em>d</em></span> must be sampled, they are given by <br /><span class="math display">$$P(\{\vec{b_i}\}_d|\{n_{\vec{b}}\}_d)=\frac{\prod_{|\vec{b_i}|=d} n_b!}{n_d!}$$</span><br /> the global posterior as defined in <span class="citation" data-cites="peixoto2015model"></span> is <br /><span class="math display">$$P(\{\vec{b_i}\}|B)=\left[\prod_{d=1}^B  P(\{\vec{b_i}\}_d|\{n_{\vec{b}}\}_d) P(\{n_{\vec{b}}\}_d|n_d)\right]P({d_i}|{n_d})P(n_d|B)$$</span><br /></p>
<p>At this time it is necessary to obtain the distribution of the edges between mixtures. Defined <span class="math inline"><em>e</em><sub><em>r</em></sub> = <em>Σ</em><sub><em>s</em></sub><em>e</em><sub><em>r</em><em>s</em></sub></span> the number of half-edges labelled <span class="math inline"><em>r</em></span>, <span class="math inline"><em>m</em><sub><em>r</em></sub> = <em>Σ</em><sub><em>b⃗</em></sub><em>b</em><sub><em>r</em></sub></span> the number of mixtures containing group <span class="math inline"><em>r</em></span> the algorithm samples the probability distribution of the edges count <br /><span class="math display">$$P(\{e_{\vec{b}}\}|\{\vec{b_i}\}, A)=\prod_r\left(\binom{m_r}{e_r}\right)^{-1}$$</span><br /> and the labeled degree sequence <span class="math inline">$\{\vec{k_i}\}$</span> from <br /><span class="math display">$$P(\{\vec{k_i}\}_{\vec{b}}|\{e_{\vec{b}}\}, \{\vec{b_i}\})=\frac{\prod_k n_k^{\vec{b}}!}{n_{\vec{b}}!}$$</span><br /></p>

<figure>
<img src="pictures/topic/peixioto_passages.png" alt="Illustration of the generative process of the microcanonical SBM. Given a partition of the nodes, the edge counts between groups are sampled (left), followed by the degrees of the nodes (center) and finally the network itself (right). From ." id="fig:hsbm-sum" /><figcaption>Illustration of the generative process of the microcanonical SBM. Given a partition of the nodes, the edge counts between groups are sampled (left), followed by the degrees of the nodes (center) and finally the network itself (right). From <span class="citation" data-cites="Peixoto2017"></span>.<span label="fig:hsbm-sum"></span></figcaption>
</figure>
<h5 id="word-documents-separation">Word documents separation</h5>
<p><br />
Following what is done in <span class="citation" data-cites="gerlach2018network"></span>, the probability of a group <span class="math inline"><em>P</em>(<em>b</em><sub><em>l</em></sub>)</span> at a certain level <span class="math inline"><em>l</em></span> is intended as the disjoint probability of group of words and group of documents: <br /><span class="math display"><em>P</em>(<em>b</em><sub><em>l</em></sub>) = <em>P</em><sub><em>w</em></sub>(<em>b</em><sub><em>l</em></sub><sup><em>w</em></sup>)<em>P</em><sub><em>d</em></sub>(<em>b</em><sub><em>l</em></sub><sup><em>d</em></sup>).</span><br /> Doing this let words and documents be separated by construction. Considering the process described above if two nodes are not connected at the beginning it is impossible that they end up in the same block. It is easily verified in <span class="citation" data-cites="peixoto2014efficient"></span> that this property is preserved and fully reflected in the final block structure.</p>
