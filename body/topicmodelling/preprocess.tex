\section{Pre-process}
This work aims to use the hierarchical Stochastic Block Model (hSBM) described before. Data considered in this work have $\sim 11000$ samples as documents and $\sim 60000$ genes as words ($\sim 20000$ if considering only protein-coding genes). The original paper~\cite{gerlach2018network} considers $63$ Wikipedia articles along with $3140$ words. The great amount of data requires some tricks to filter the network and make the computation faster.
Different approaches were tested to pre-process the data, all of them involving the quantities defined in chapter~\ref{ch:structure}. The goal is to identify components which can best separate the realizations or, in other words, isolate the most interesting genes.
 
\paragraph{Low occurrence genes} were selected firstly to approach topic modelling. A $0.5$ threshold was set on occurrence. This method selects genes that appear (have expression greater than zero) only in less than half samples. This approach has some limitations, for instance it doesn't consider genes that appear everywhere (with occurrence $\simeq 1$) but changes their behaviour across realizations.

\paragraph{Tf-idf (term frequencyâ€“inverse document frequency)} should help. This approach doesn't take into account original expression values $n_{ij}$, but a transformed version
\[
n^{new}_{ij}=\frac{n_{i j}}{M_j}\times \left(1-Log\left(o_i\right)\right)
\] which increases the importance of components with small occurrence $o_i$. This is widely used in linguistics to trash out common words. Tf-idf doesn't select or reduce the number of components (genes), which is still an issue.

\paragraph{Highly variable} genes can be selected. This is done using the $CV^2$ analysis done in chapter~\ref{ch:scalinglaws}.
\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\linewidth]{pictures/topic/cvmean_oversigma.png}
    \caption{Highly variable genes. In cyan genes that are $1 \sigma$ over the average of their bin.}
    \label{fig:topic/cvmean_oversigma}
\end{figure}
Plotting the coefficient of variation versus the mean, one point for each component, reveals components that have higher variance than components which, on average, have similar behaviour. Binned averages and variances were estimated. Only genes with a $CV^2$ over a $\sigma$ were considered. This method seems useful to select genes even if the binned average bound is quite noisy.

\paragraph{Distance from boundaries} can be a similar and alternative method to select highly variable genes. In this case, the boundary is both smooth and well-defined.
\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\linewidth]{pictures/topic/cvmean_oversampling.png}
    \caption{In cyan genes which distance from boundaries is greater than $10 CV^2$.}
    \label{fig:topic/cvmean_oversampling}
\end{figure}
The distribution as discussed in~\ref{ch:scalinglaws} have Poisson-like and Taylor-like boundaries. So it is possible to consider only components that are the most distant from these boundaries. Moreover, these boundaries can be found with a simple null model, as shown in figure~\ref{fig:scalinglaws/gtex/cvmean_loglog_sampling} the sampling model defines the lower bound of the data.

The last two approaches are the ones which lead to better results, in the following section gene selection was done by getting only highly variables genes.
To reduce the number of documents, samples are picked up uniformly random from a subset of all the available ones.